{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로봇의 샌드위치 탐색 시뮬레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 진행 방법\n",
    "1. MDP 구성 요소 정의\n",
    "2. 파이썬으로 MDP 모델링\n",
    "3. 가치 반복 알고리즘 구현\n",
    "4. 시뮬레이션 및 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태와 행동 정의\n",
    "states = ['Hungry', 'Still Hungry', 'Full']\n",
    "actions = ['Go to Fridge', 'Go to Subway']\n",
    "\n",
    "# 상태 전이 확률 정의\n",
    "P = {\n",
    "    'Hungry': {\n",
    "        'Go to Fridge': {'Still Hungry': 0.8, 'Full': 0.2},\n",
    "        'Go to Subway': {'Full': 1.0}\n",
    "    },\n",
    "    'Still Hungry': {\n",
    "        'Go to Subway': {'Full': 1.0}\n",
    "    },\n",
    "    'Full': {}\n",
    "}\n",
    "\n",
    "# 보상 함수 정의\n",
    "R = {\n",
    "    'Hungry': {\n",
    "        'Go to Fridge': {'Still Hungry': -1, 'Full': 99},\n",
    "        'Go to Subway': {'Full': 30}\n",
    "    },\n",
    "    'Still Hungry': {\n",
    "        'Go to Subway': {'Full': 30}\n",
    "    },\n",
    "    'Full': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 할인율\n",
    "gamma = 0.9\n",
    "\n",
    "# 가치 함수 및 정책 초기화\n",
    "V = {s:0 for s in states}\n",
    "policy = {s: None for s in states}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  State Hungry, Action Go to Fridge: Value 19.00\n",
      "  State Hungry, Action Go to Subway: Value 30.00\n",
      "  State Still Hungry, Action Go to Subway: Value 30.00\n",
      "    Updated Values: {'Hungry': 30.0, 'Still Hungry': 30.0, 'Full': 0}\n",
      "    Updated Policy: {'Hungry': 'Go to Subway', 'Still Hungry': 'Go to Subway', 'Full': None}\n",
      "\n",
      "Iteration 1\n",
      "  State Hungry, Action Go to Fridge: Value 40.60\n",
      "  State Hungry, Action Go to Subway: Value 30.00\n",
      "  State Still Hungry, Action Go to Subway: Value 30.00\n",
      "    Updated Values: {'Hungry': 40.6, 'Still Hungry': 30.0, 'Full': 0}\n",
      "    Updated Policy: {'Hungry': 'Go to Fridge', 'Still Hungry': 'Go to Subway', 'Full': None}\n",
      "\n",
      "Iteration 2\n",
      "  State Hungry, Action Go to Fridge: Value 40.60\n",
      "  State Hungry, Action Go to Subway: Value 30.00\n",
      "  State Still Hungry, Action Go to Subway: Value 30.00\n",
      "    Updated Values: {'Hungry': 40.6, 'Still Hungry': 30.0, 'Full': 0}\n",
      "    Updated Policy: {'Hungry': 'Go to Fridge', 'Still Hungry': 'Go to Subway', 'Full': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 가치 반복 알고리즘 구현\n",
    "theta = 0.0001  # 수렴 임계값\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    print(f'Iteration {iteration}')\n",
    "    for s in states:\n",
    "        v = V[s]\n",
    "        action_values = {}\n",
    "        for a in actions:\n",
    "            if a in P[s]:\n",
    "                total = 0\n",
    "                for s_prime in P[s][a]:\n",
    "                    prob = P[s][a][s_prime]\n",
    "                    reward = R[s][a][s_prime]\n",
    "                    total += prob * (reward + gamma * V[s_prime])\n",
    "                action_values[a] = total\n",
    "                print(f'  State {s}, Action {a}: Value {total:.2f}')\n",
    "        if action_values:\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            V[s] = action_values[best_action]\n",
    "            policy[s] = best_action\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "    iteration += 1\n",
    "    print(f\"    Updated Values: {V}\")\n",
    "    print(f\"    Updated Policy: {policy}\\n\")\n",
    "    if delta < theta:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 가치 함수:\n",
      "State Hungry: 40.6\n",
      "State Still Hungry: 30.0\n",
      "State Full: 0\n",
      "\n",
      "최적 정책:\n",
      "State Hungry: Go to Fridge\n",
      "State Still Hungry: Go to Subway\n",
      "State Full: None\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "print('최종 가치 함수:')\n",
    "for s in states:\n",
    "    print(f\"State {s}: {V[s]}\")\n",
    "    \n",
    "print('\\n최적 정책:')\n",
    "for s in states:\n",
    "    print(f'State {s}: {policy[s]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
