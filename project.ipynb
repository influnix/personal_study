{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프라이버시 안전한 데이터셋을 활용한 고품질 이미지 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비\n",
    "1. 고품질 이미지 수집\n",
    "2. 열화 이미지 생성\n",
    "3. 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 디렉토리 설정\n",
    "hq_image_dir = './data/archive/train/'\n",
    "lq_image_dir = './data/unsplash_images/lq/train/'\n",
    "os.makedirs(lq_image_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열화 함수 정의\n",
    "def degrade_image(image):\n",
    "    # 랜덤으로 열화 적용\n",
    "    if random.random() < 0.5:\n",
    "        # 가우시안 블러\n",
    "        radius = random.uniform(1, 3)\n",
    "        image = image.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "    if random.random() < 0.5:\n",
    "        # 노이즈 추가\n",
    "        noise = np.random.normal(0, 25, (image.size[1], image.size[0], 3))\n",
    "        noise = Image.fromarray(noise.astype('uint8'), 'RGB')\n",
    "        image = Image.blend(image, noise, alpha=0.5)\n",
    "    if random.random() < 0.5:\n",
    "        # JPEG 압축\n",
    "        try:\n",
    "            image.save('temp.jpg', 'JPEG', quality=random.randint(10, 50))\n",
    "            with Image.open('temp.jpg') as degraded_image:\n",
    "                image = degraded_image.copy()\n",
    "        finally:\n",
    "            if os.path.exists('temp.jpg'):\n",
    "                os.remove('temp.jpg')\n",
    "    if random.random() < 0.5:\n",
    "        # 해상도 저하\n",
    "        scale_factor = random.uniform(0.5, 0.8)\n",
    "        new_size = (int(image.size[0] * scale_factor), int(image.size[1]*scale_factor))\n",
    "        image = image.resize(new_size, Image.BICUBIC)\n",
    "        image = image.resize(image.size, Image.BICUBIC)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LQ 이미지 생성\n",
    "for filename in os.listdir(hq_image_dir):\n",
    "    hq_image_path = os.path.join(hq_image_dir, filename)\n",
    "    lq_image_path = os.path.join(lq_image_dir, filename)\n",
    "    image = Image.open(hq_image_path).convert('RGB')\n",
    "    degraded_image = degrade_image(image)\n",
    "    degraded_image.save(lq_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDegradationDataset(Dataset):\n",
    "    def __init__(self, hq_dir, lq_dir, transform=None):\n",
    "        self.hq_dir = hq_dir\n",
    "        self.lq_dir = lq_dir\n",
    "        self.hq_images = sorted(os.listdir(hq_dir))\n",
    "        self.lq_images = sorted(os.listdir(lq_dir))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hq_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hq_path = os.path.join(self.hq_dir, self.hq_images[idx])\n",
    "        lq_path = os.path.join(self.lq_dir, self.lq_images[idx])\n",
    "        \n",
    "        hq_image = Image.open(hq_path).convert('RGB')\n",
    "        lq_image = Image.open(lq_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            hq_image = self.transform(hq_image)\n",
    "            lq_image = self.transform(lq_image)\n",
    "            \n",
    "        return lq_image, hq_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDegradationDataset(hq_dir=hq_image_dir, lq_dir=lq_image_dir, transform=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 설계\n",
    "1. 네트워크 구조 선택\n",
    "2. Mixture of Experts(MoE) 구조 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDSR(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_feats=64, num_blocks=16, num_experts=4):\n",
    "        super(EDSR, self).__init__()\n",
    "        self.head = nn.Conv2d(num_channels, num_feats, kernel_size=3, padding=1)\n",
    "        self.body = nn.ModuleList([ResidualBlock(num_feats) for _ in range(num_blocks)])\n",
    "        self.tail = nn.Conv2d(num_feats, num_channels, kernel_size=3, padding=1)\n",
    "        self.num_experts = num_experts\n",
    "        self.router = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_experts, kernel_size=1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.experts = nn.ModuleList([ResidualBlock(num_feats) for _ in range(num_experts)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.head(x)\n",
    "        # 라우터를 통해 전문가 가중치 계산\n",
    "        weights = self.router(x)\n",
    "        # 각 전문가의 출력 계산 및 가중 합산\n",
    "        expert_out = 0\n",
    "        for i in range(self.num_experts):\n",
    "            expert_feat = self.sxperts[i](feat)\n",
    "            weight = weights[:, i:i+1, :, :]\n",
    "            expert_out += expert_feat * weight\n",
    "        res = expert_out\n",
    "        for block in self.body:\n",
    "            res = block(res)\n",
    "        res += feat\n",
    "        out = self.tail(res)\n",
    "        return out\n",
    "    \n",
    "model = EDSR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 학습\n",
    "1. 손실 함수 및 옵타마이저 설정\n",
    "2. 학습 루프 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\miniconda3\\envs\\portfolio\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\miniconda3\\envs\\portfolio\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Perceptual Loss를 위한 VGG 모델\n",
    "vgg = models.vgg19(pretrained=True).features[:35].eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "def perceptual_loss(output, target):\n",
    "    output_vgg = vgg(output)\n",
    "    target_vgg = vgg(target)\n",
    "    loss = nn.functional.l1_loss(output_vgg, target_vgg)\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for lq_imgs, hq_imgs in train_loader:\n",
    "        lq_imgs = lq_imgs.to(device)\n",
    "        hq_imgs = hq_imgs.to(device)\n",
    "        outputs = model(lq_imgs)\n",
    "        loss1 = criterion(outputs, hq_imgs)\n",
    "        loss2 = perceptual_loss(outputs, hq_imgs)\n",
    "        loss = loss1 + 0.01 * loss2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 평가\n",
    "1. 평가 지표 계산\n",
    "2. 테스트 데이터셋을 사용하여 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lpips import LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips_loss_fn = LPIPS(net='alex').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_psnr = 0\n",
    "total_ssim = 0\n",
    "total_lpips = 0\n",
    "with torch.no_grad():\n",
    "    for lq_imgs, hq_imgs in test_loader:\n",
    "        lq_imgs = lq_imgs.to(device)\n",
    "        hq_imgs = lq_imgs.to(device)\n",
    "        outputs = model(lq_imgs)\n",
    "        # PSNR 계산\n",
    "        mse = torch.mean((outputs - hq_imgs) ** 2)\n",
    "        psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "        total_psnr += psnr.item()\n",
    "        # SSIM 계산\n",
    "        ssim = pytorch_ssim.ssim(outputs, hq_imgs).item()\n",
    "        total_ssim += ssim\n",
    "        # LPIPS 계산\n",
    "        lpips_value = lpips_loss_fn(outputs, hq_imgs).mean().item()\n",
    "        total_lpips += lpips_value\n",
    "avg_psnr = total_psnr / len(test_loader)\n",
    "avg_ssim = total_ssim / len(test_loader)\n",
    "avg_lpips = total_lpips / len(test_loader)\n",
    "print(f'Average PSNR: {avg_psnr:.2f} dB')\n",
    "print(f'Average SSIM: {avg_ssim:.4f}')\n",
    "print(f'Average LPIPS: {avg_lpips:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결과 분석 및 개선\n",
    "1. 결과 시각화\n",
    "2. 모델 개선 방안 제시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataiter = iter(test_loader)\n",
    "lq_imgs, hq_imgs = next(dataiter)\n",
    "lq_imgs = lq_imgs.to(device)\n",
    "hq_imgs = hq_imgs.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(lq_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 이미지 시각화\n",
    "idx = 0\n",
    "lq_img = lq_imgs[idx].cpu().permute(1, 2, 0).numpy()\n",
    "output_img = outputs[idx].cpu().permute(1, 2, 0).numpy()\n",
    "hq_img = hq_imgs[idx].cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Low-Quality Image')\n",
    "plt.imshow(lq_img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Restored Image')\n",
    "plt.imshow(output_img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('High-Quality Image')\n",
    "plt.imshow(hq_img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
